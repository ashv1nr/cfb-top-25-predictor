{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "The machine learning problem we are trying to solve is predicting the College Football Playoff Commitee's top 25 teams ranking. This matters beacuse th CFP Commitee's rankings are riddled with controversey year after year, as teams are constantly frustrated with their ranking. These rankings matter heavily, as they determine how prestigious of a bowl game teams play in, as well as who gets to compete for a national champsionship. There is very little information on what the commitee considers when they rank the teams, so our model could be used to give teams insight into what particular statistics the committe might value the most when they determine which teams should be ranked higher than others. Teams could then place emphasis on say making sure they have good passing offense or a positive turnover margin if the model shows teams who perform well in those categories are ranked well by the commitee.\n",
    "\n",
    "The dataset we are using we got from Kaggle at the following link: https://shorturl.at/glT68\n",
    "\n",
    "It holds data on over a 140 statistical categories(our features) on all FBS teams(the best 130 or so teams in the country) for every year the College Playoff Committe has existed, which is from 2014 to the present. Examples of features in our dataset are total points scored, offensive yards per play, sacks, and many more. We have over 800 records in our dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkYcOMpSab7H"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_year = 14 # the first dataset is from 2014\n",
    "num_years = 7 # the datasets go from 2014-2020\n",
    "df_dict = {} # holds individual dataframes for each year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load every dataset into a dataframe and maps the year to its corresponding dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ikXrOv4BuZWb"
   },
   "outputs": [],
   "source": [
    "for year in range(first_year, first_year + num_years):\n",
    "    df = pd.read_csv(\"cfb\" + str(year) + \".csv\")\n",
    "    df_dict[year] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adds the year the datasets are from as a column in each dataframe. We will drop the year feature eventually, but it will be helpful when we feature engineer later after merging the dataframes into one big dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(first_year, first_year + num_years):\n",
    "    df = df_dict[year]\n",
    "    df['year'] = year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were some issues with the kick return data being in wrong columns in some datasets, so we swapped around the data and fixed some column name issues as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(first_year + 2, first_year + num_years):\n",
    "    df = df_dict[year]\n",
    "    temp_data = df['Avg.Yard.per.Kickoff.Return'].copy()\n",
    "    df['Avg.Yard.per.Kickoff.Return'] = df['...40']\n",
    "    df['...40'] = temp_data\n",
    "    temp_data = df['Kickoff.Return.Touchdowns'].copy()\n",
    "    df['Kickoff.Return.Touchdowns'] = df['...40']\n",
    "    df['...40'] = temp_data\n",
    "    temp_data = df['Kickoff.Return.Yards'].copy()\n",
    "    df['Kickoff.Return.Yards'] = df['...40']\n",
    "    df['extra column'] = temp_data\n",
    "    df = df.drop(columns='...40')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merges each individual dataframe into one big dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_set = sorted(list(df_dict.keys()))\n",
    "while len(key_set) > 1:\n",
    "  df0 = df_dict[key_set[0]]\n",
    "  df1 = df_dict[key_set[1]]\n",
    "  shared_columns = df0.columns.intersection(df1.columns)\n",
    "  df0 = df0.loc[:, shared_columns]\n",
    "  df1 = df1.loc[:, shared_columns]\n",
    "  df0 = pd.concat([df0, df1], ignore_index=True)\n",
    "  df_dict[key_set[0]] = df0\n",
    "  key_set.remove(key_set[1])\n",
    "df = df_dict[14] # our one big dataframe after merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two University of Miami's in our datasets. One in Florida and another in Ohio. We wanted to strip the parentheses off the location to help for a later step which also deals with parsing info out of parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJYqoDPdepsb"
   },
   "outputs": [],
   "source": [
    "df['Team'] = df['Team'].str.replace('Miami \\(FL\\)', 'Miami FL', regex=True)\n",
    "df['Team'] = df['Team'].str.replace('Miami \\(OH\\)', 'Miami OH', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets mistakenly mixed up the data for two features. Data for the number of 4th down attempts a team had was placed under the feature for the number of 4th down conversions that team had and vice versa. So we need to swap the data to the correct columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = df['X4th.Attempts'].copy()\n",
    "df['X4th.Attempts'] = df['X4th.Conversions']\n",
    "df['X4th.Conversions'] = temp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Exporation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wanted to examine if the notion on college football becoming more offensive is actually true. Offensive yards per game and total touchdowns are good measures of offensive productivity, so we can graph the yearly averages of these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.groupby('year')['Off.Yards.per.Game'].mean()\n",
    "data.plot(kind='bar')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('average offensive yards per game')\n",
    "plt.show()\n",
    "data = df.groupby('year')['Points.Per.Game'].mean()\n",
    "data.plot(kind='bar')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('average points per game')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were also curious in validating on if passing is becmoing more prominent and the value of rushing is fading. Pass yards per game and rush yards per game can give us a good idea on this topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.groupby('year')[['Pass.Yards.Per.Game', 'Rushing.Yards.per.Game']].mean()\n",
    "data.plot(kind='bar', stacked=True)\n",
    "plt.xlabel('year?')\n",
    "plt.ylabel('distribution of total yards')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Staying with the theme on if teams are becoming more aggressive offensively, we can analyze the average amount of 4th down attempts teams are taking each year. The dramatic drop in 2020 is due to Covid-19 impacting the number of games many teams played that year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.groupby('year')['X4th.Attempts'].mean()\n",
    "data.plot(kind='line')\n",
    "plt.xlabel('year?')\n",
    "plt.ylabel('average 4th down attempts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concern for players' health is a growing trend in sports nowdays. Defense is where players exhaust alot of energy, so we were interested if there is any trend in the number of defensive snaps teams are taking each year. The dramatic drop in 2020 is due to Covid-19 impacting the number of games many teams played that year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.groupby('year')['Def.Plays'].mean()\n",
    "data.plot(kind='line')\n",
    "plt.xlabel('year?')\n",
    "plt.ylabel('average defensive plays')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There has also been a movement to protect the quarterback more, which has led to tackles on the quarterback sometimes being ruled a penalty instead of a sack for the defense. We can graph the yearly averages of penalty yards per game and sacks per game to see if its true that penalties are increasing due to new rules, resulting in less sacks for the defense, or if the extent of that is overblown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.groupby('year')['Penalty.Yards.Per.Game'].mean()\n",
    "data.plot(kind='line')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('average penalty yards per game')\n",
    "plt.show()\n",
    "data = df.groupby('year')['Average.Sacks.per.Game'].mean()\n",
    "data.plot(kind='line')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('average sacks per game')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also staying on the topic of health, there have been rules put in place to deincentivize teams from trying to return kickoffs as they can be one of the most dangerous plays in football due to players running full speed at each other from oppositie sides of the field. We wanted to examine if there is an observable decrease following these new rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.groupby('year')['Kickoff.Return.Yards'].mean()\n",
    "data.plot(kind='line')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('average kick return yeards')\n",
    "plt.show()\n",
    "data = df.groupby('year')['Kickoffs.Returned'].mean()\n",
    "data.plot(kind='line')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('average kickoffs returned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are features that record the number of pass attempts and and pass completions a team had. We can make a completion percentage feature out of this to express the odds a pass will be succesfully completed when a team chooses to throw. \n",
    "\n",
    "We will drop the number of completions since we now have this completion percentage feature, but we will keep the number of pass attempts a team has since that describes their play style, which could determine how successful they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['complt perct'] = df['Pass.Completions'] / df['Pass.Attempts']\n",
    "df = df.drop(columns='Pass.Completions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are features that record the number of pass attempts faced and and pass completions allowed a team had. We can make an opponent completion percentage feature out of this to express the odds a pass will be succesfully completed against this team when they face a pass.\n",
    "\n",
    "We will drop the number of completions allowed since we now have this opponent completion percentage feature, but we will keep the number of pass attempts a team faces since that describes how opponents play against this team, which could determine how successful they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['opp complt perct'] = df['Opp.Completions.Allowed'] / df['Opp.Pass.Attempts']\n",
    "df = df.drop(columns='Opp.Completions.Allowed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best teams when they are in the redzone(the last 20 yards before the endzone) score touchdowns(6 points) when they are that close opposed to settling for field goals(3 points) or not scoring at all. We can make a feature that expresses how often a team scores a touchdown when they are in the redzone. \n",
    "\n",
    "We will keep the amount of redzone attempts feature beacuse that tells us how many times a team gets into a close scoring poistion, which can say alot about how good or bad a team is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['redzone td perct'] = (df['Redzone.Rush.TD'] + df['Redzone.Pass.TD']) / df['Redzone.Attempts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to also know on what percentage of trips to the redzone a team settles for a field goal, so we can create a feature for that.\n",
    "\n",
    "We can then drop the feature for the number of redzone field goals a team made after creating our redzone filed goal percentage feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['redzone fg perct'] = df['Redzone.Field.Goals.Made'] / df['Redzone.Attempts']\n",
    "df = df.drop(columns='Redzone.Field.Goals.Made')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then lastly make a feature to express the percentage of trips to the redzone a teams fails to score using the features we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['redzone empty perct'] = 1 - df['redzone td perct'] - df['redzone fg perct']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply all the same logic above to create features for the opponents redzone statistis which can give us an idea of how good this teams' redzone defense is and if they can force their opponent into settling for field goals instead of scoring touchdowns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['opp redzone td perct'] = (df['Opp.Redzone.Rush.TD.Allowed'] + df['Opp.Redzone.Pass.Touchdowns.Allowed']) / df['Opp.Redzone.Attempts']\n",
    "df['opp redzone fg perct'] = df['Opp.Redzone.Field.Goals.Made'] / df['Opp.Redzone.Attempts']\n",
    "df = df.drop(columns='Opp.Redzone.Field.Goals.Made')\n",
    "df['opp redzone empty perct'] = 1 - df['opp redzone td perct'] - df['opp redzone fg perct']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big plays are very critical to have as an offense and limit as a defense in football. We can create features expressing for a team the number of touchdowns they scored and allowed outside the redzone(scores of over 20 yards).\n",
    "\n",
    "We can subsequently drop the amount of redzone rush and pass touchdowns a team has had and allowed after creating the redzone touchdown percentage feature above and this non redzone touhchdown feature below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['non redzone td'] = df['Off.TDs'] - (df['Redzone.Rush.TD'] + df['Redzone.Pass.TD'])\n",
    "df['non redzone td allowed'] = df['Off.TDs.Allowed'] - (df['Opp.Redzone.Rush.TD.Allowed'] + df['Opp.Redzone.Pass.Touchdowns.Allowed'])\n",
    "columns_to_drop = ['Redzone.Rush.TD', 'Redzone.Pass.TD', 'Opp.Redzone.Rush.TD.Allowed', 'Opp.Redzone.Pass.Touchdowns.Allowed']\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of games teams play can vary based on if they qualified for their conference championship or not, and during the Covid-19 pandemic, some conferences played a drastically different amount of games than others. For these reasons, it would make more sense to use win percentage rather than just number of wins in our model.\n",
    "\n",
    "We can subsequently drop the number of wins, lossess, and games played from our dataframe after creating our win percentage feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wukLabE8HmLP"
   },
   "outputs": [],
   "source": [
    "df['win perct'] = df['Win'] / df['Games']\n",
    "columns_to_drop = ['Win', 'Loss', 'Games']\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conference a team is plays a big role in how the commitee ranks them. Most of the games teams play are against teams in the same conference, and some conferences have historically had higher performing teams. As a result, teams in these more elite conferences often have harder schedules, and thus it is critical to take into consideration the conference a team plays in when ranking them.\n",
    "\n",
    "In our datasets, the conference is not its own feature, it is attached to the feature for the team's name. Hence, we parse the conference from the team's name and add a column for the conference the team is in.\n",
    "\n",
    "After we parse the conference from the team name feature and create the conference feature, we can remove the conference from the team name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U15jotVuN-Bz"
   },
   "outputs": [],
   "source": [
    "df['conference'] = df['Team'].apply(lambda x: x[x.find('(') + 1: x.find(')')] if '(' in x and ')' in x else x)\n",
    "df['Team'] = df['Team'].apply(lambda x: x[:x.find('(')] if '(' in x else x)\n",
    "df['Team'] = df['Team'].str[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were a couple individual records with missing/wrong conferences, so we manually set the teams' conference to its correct one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XdiGPrZ3qQKe"
   },
   "outputs": [],
   "source": [
    "df.loc[df['conference'] == 'Independent', 'conference'] = 'FBS Independent'\n",
    "df.loc[(df['conference'] == '') & (df['Team'] == 'Ole Miss'), 'conference'] = 'SEC'\n",
    "df.loc[(df['conference'] == '') & (df['Team'] == 'Pittsburgh'), 'conference'] = 'ACC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our class label is if the team was ranked in the top 25 or not for the given year, so we utilize the lists below to add a column which holds a 1 if the team was ranked in the top 25 that year and a 0 if not.\n",
    "\n",
    "After we create the feature that says if a team was in the top 25 or not for the given year, we can drop the year column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85JMF9n9T_Rw"
   },
   "outputs": [],
   "source": [
    "# lists of the CFP Commitee's top 25 ranking each year\n",
    "\n",
    "top25_14 = ['Alabama', 'Oregon', 'Florida St.', 'Ohio St.', 'Baylor',\n",
    "            'TCU', 'Mississippi St.', 'Michigan St.', 'Ole Miss', 'Arizona',\n",
    "            'Kansas St.', 'Georgia Tech', 'Georgia', 'UCLA', 'Arizona St.',\n",
    "            'Missouri', 'Clemson', 'Wisconsin', 'Auburn', 'Boise St.',\n",
    "            'Louisville', 'Utah', 'LSU', 'Southern California', 'Minnesota'] # top 25 from 2014\n",
    "\n",
    "top25_15 = ['Clemson', 'Alabama', 'Michigan St.', 'Oklahoma', 'Iowa',\n",
    "            'Stanford', 'Ohio St.', 'Notre Dame', 'Florida St.', 'North Carolina',\n",
    "            'TCU', 'Ole Miss', 'Northwestern', 'Michigan', 'Oregon',\n",
    "            'Oklahoma St.', 'Baylor', 'Houston', 'Florida', 'LSU',\n",
    "            'Navy', 'Utah', 'Tennessee', 'Temple', 'Southern California'] # top 25 from 2015\n",
    "\n",
    "top25_16 = ['Alabama', 'Clemson', 'Ohio St.', 'Washington', 'Penn St.',\n",
    "            'Michigan', 'Oklahoma', 'Wisconsin', 'Southern California', 'Colorado',\n",
    "            'Florida St.', 'Oklahoma St.', 'Louisville', 'Auburn', 'Western Mich.',\n",
    "            'West Virginia', 'Florida', 'Stanford', 'Utah', 'LSU',\n",
    "            'Tennessee', 'Virginia Tech', 'Pittsburgh', 'Temple', 'Navy'] # top 25 from 2016\n",
    "\n",
    "top25_17 = ['Clemson', 'Oklahoma', 'Georgia', 'Alabama', 'Ohio St.',\n",
    "            'Wisconsin', 'Auburn', 'Southern California', 'Penn St.', 'Miami FL',\n",
    "            'Washington', 'UCF', 'Stanford', 'Notre Dame', 'TCU',\n",
    "            'Michigan St.', 'LSU', 'Washington St.', 'Oklahoma St.', 'Memphis',\n",
    "            'Northwestern', 'Virginia Tech', 'Mississippi St.', 'NC State', 'Boise St.'] # top 25 from 2017\n",
    "\n",
    "top25_18 = ['Alabama', 'Clemson', 'Notre Dame', 'Oklahoma', 'Georgia',\n",
    "            'Ohio St.', 'Michigan', 'UCF', 'Washington', 'Florida',\n",
    "            'LSU', 'Penn St.', 'Washington St.', 'Kentucky', 'Texas',\n",
    "            'West Virginia', 'Utah', 'Mississippi St.', 'Texas A&M', 'Syracuse',\n",
    "            'Fresno St.', 'Northwestern', 'Missouri', 'Iowa St.', 'Boise St.'] # top 25 from 2018\n",
    "\n",
    "top25_19 = ['LSU', 'Ohio St.', 'Clemson', 'Oklahoma', 'Georgia',\n",
    "            'Oregon', 'Baylor', 'Wisconsin', 'Florida', 'Penn St.',\n",
    "            'Utah', 'Auburn', 'Alabama', 'Michigan', 'Notre Dame',\n",
    "            'Iowa', 'Memphis', 'Minnesota', 'Boise St.', 'Appalachian St.',\n",
    "            'Cincinnati', 'Southern California', 'Navy', 'Virginia', 'Oklahoma St.'] # top 25 from 2019\n",
    "\n",
    "top25_20 = ['Alabama', 'Clemson', 'Ohio St.', 'Notre Dame', 'Texas A&M',\n",
    "            'Oklahoma', 'Florida', 'Cincinnati', 'Georgia', 'Iowa St.',\n",
    "            'Indiana', 'Coastal Carolina', 'North Carolina', 'Northwestern', 'Iowa',\n",
    "            'BYU', 'Southern California', 'Miami FL', 'Louisiana', 'Texas',\n",
    "            'Oklahoma St.', 'San Jose St.', 'NC State', 'Tulsa', 'Oregon'] # top 25 from 2020\n",
    "\n",
    "top25_dict = {14: top25_14, 15: top25_15, 16: top25_16, 17: top25_17, 18: top25_18,\n",
    "              19: top25_19, 20: top25_20} # dictionary to be able to grab rankings for a desired year\n",
    "\n",
    "df['top 25'] = df.apply(lambda row: 1 if row['Team'] in top25_dict.get(row['year'], []) else 0, axis=1)\n",
    "df = df.drop(columns=['year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below features are being dropped beacause there is already a feature in our dataframe that is a calculation of these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Off.Plays', 'Def.Plays', 'X4th.Conversions', 'Opp.4th.Conversion',\n",
    "                   'Kickoffs.Returned', 'Kickoff.Return.Yards', 'Punt.Returns', 'Net.Punt.Return.Yards',\n",
    "                   'Opp.Punt.Returns', 'Opp.Net.Punt.Return.Yards', 'Redzone.Points', 'Redzone.Points.Allowed',\n",
    "                   'X3rd.Attempts', 'X3rd.Conversions', 'Opp.3rd.Attempt', 'Opp.3rd.Conversion']\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show why we would want to have a feature that says what conference a team is in, the below plot displays the distribution of ranked teams based on what conference they are in. As we can see, teams in the top 25 primarily come from a couple conferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.groupby('conference')['top 25'].sum()\n",
    "data = data.sort_values(ascending=False)\n",
    "data.plot(kind='bar')\n",
    "plt.xlabel('conference')\n",
    "plt.ylabel('number of teams in top 25 from 2014-2020')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis of conferences can be taken further by looking at the distribution of the top 25 based on conference to see on average what proportion of ranked teams come from each conference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.groupby('conference')['top 25'].sum()\n",
    "data = data.sort_values(ascending=False)\n",
    "data = data / num_years\n",
    "data.plot(kind='pie')\n",
    "plt.title('proportion of the top 25 that is made up by each conference on average')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few important features we engineered were the percenatges of how teams scored points when they were in the redzone, so we can plot the average percentage of time ranked versus unranked teams score touchdowns, field goals, or come up empty when they are in the redzone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.groupby('top 25')[['redzone td perct', 'redzone fg perct', 'redzone empty perct']].mean()\n",
    "ax = data.plot(kind='bar', stacked=True)\n",
    "plt.xlabel('top 25?')\n",
    "plt.ylabel('avg redzone percentages')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most talked-about statistics in the media is the turnover margin. The turnover margin is the difference between the number of times a team took the ball away from their opponent and the number of times they gave the ball to their opponent. Good teams are known to have positive turnover margins, and we wanted to confirm that by plotting the average turnover margin for ranked teams versus those teams not in the top 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.groupby('top 25')['Turnover.Margin'].mean()\n",
    "data.plot(kind='bar')\n",
    "plt.xlabel('top 25?')\n",
    "plt.ylabel('avg turnover margin')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(df['conference'])\n",
    "one_hot = one_hot.astype('int')\n",
    "#df = pd.get_dummies(df, columns = ['conference'])\n",
    "df = df.drop('conference', axis = 1)\n",
    "df.join(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_string_df = df.drop(['Team'], axis=1)\n",
    "\n",
    "def convert_time(str_time):\n",
    "    minutes, seconds = str_time.split(':')\n",
    "    return (int(minutes) * 60) + int(seconds)\n",
    "\n",
    "no_string_df['Time.of.Possession'] = no_string_df['Time.of.Possession'].apply(lambda x: convert_time(x))\n",
    "no_string_df['Average.Time.of.Possession.per.Game'] = no_string_df['Average.Time.of.Possession.per.Game'].apply(lambda x: convert_time(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "scaled_df = scaler.fit_transform(no_string_df)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=no_string_df.columns)\n",
    "scaled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = no_string_df.drop('top 25', axis=1)\n",
    "labels = no_string_df['top 25'].values.ravel()\n",
    "scaled_features = scaled_df.drop('top 25', axis=1)\n",
    "scaled_labels = scaled_df['top 25'].values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees with different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "param_grid = {'max_depth': [5, 10, 15, 20], 'min_samples_leaf': [5, 10, 15, 20], 'max_features': [5, 10, 15]}\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
    "nested_scores = cross_val_score(grid_search, features, labels, cv=5)\n",
    "print(\"Average accuracy:\", nested_scores.mean() * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "scores = cross_val_score(gnb, features, labels, cv=10, scoring='accuracy')\n",
    "print(\"Average accuracy:\", scores.mean() * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "param_grid = {'pca__n_components': list(range(5, 19)), 'knn__n_neighbors': list(range(1, 25))}\n",
    "\n",
    "gr_sch = GridSearchCV(pl, param_grid, cv = 5)\n",
    "gr_sch.fit(features, labels)\n",
    "outer_loop = KFold(n_splits = 5, shuffle = True, random_state = 21)\n",
    "nested_scores = cross_val_score(gr_sch, features, labels, cv = outer_loop)\n",
    "accuracy = nested_scores.mean()\n",
    "print(\"Nested CV accuracy:\", accuracy * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_pl = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n",
    "prm_grd = { 'svc__C': [0.1, 1, 10, 100], 'svc__kernel': ['linear', 'rbf', 'poly']}\n",
    "gr_sch = GridSearchCV(svm_pl, prm_grd, cv = 5)\n",
    "predictions = cross_val_predict(gr_sch, features, labels, cv = outer_loop)\n",
    "accuracy = cross_val_score(gr_sch, features, labels, cv = 5, scoring = 'accuracy').mean()\n",
    "print(\"Accuracy:\", accuracy * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def run_nn():\n",
    "    pl = Pipeline([('scaler', StandardScaler()), ('mlp', MLPClassifier(max_iter = 1000, random_state = 21))])\n",
    "    prm_grd = {'mlp__hidden_layer_sizes': [(30,), (40,), (50,), (60,)], 'mlp__activation': ['logistic', 'tanh', 'relu']}\n",
    "    grid_search = GridSearchCV(pl, prm_grd, cv=5)\n",
    "    accuracy = cross_val_score(grid_search, features, labels, cv = 5, scoring = 'accuracy').mean()\n",
    "    print(\"Accuracy:\", accuracy * 100, \"%\")\n",
    "  \n",
    "\n",
    "run_nn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "gnb = GaussianNB()\n",
    "predicted_labels = cross_val_predict(gnb, features, labels, cv=10)\n",
    "rf_clf = RandomForestClassifier(random_state = 21)\n",
    "prm_grd = {'n_estimators': [50, 100, 150]}\n",
    "grd_sch = GridSearchCV(rf_clf, prm_grd, cv = 5)\n",
    "predictions = cross_val_predict(grd_sch, features, labels, cv = 5)\n",
    "cls_rpt = classification_report(labels, predicted_labels)\n",
    "print(cls_rpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
